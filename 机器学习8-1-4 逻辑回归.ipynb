{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.datasets import load_breast_cancer \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.990</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.71190</td>\n",
       "      <td>0.26540</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.570</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.070170</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.24160</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.690</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.45040</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.420</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.68690</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.290</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>0.16250</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.080890</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>15.470</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.52490</td>\n",
       "      <td>0.53550</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.250</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>22.880</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.14420</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.37840</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.710</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>17.060</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.16540</td>\n",
       "      <td>0.36820</td>\n",
       "      <td>0.26780</td>\n",
       "      <td>0.15560</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.000</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.093530</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>15.490</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.17030</td>\n",
       "      <td>0.54010</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.460</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.085430</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>15.090</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.18530</td>\n",
       "      <td>1.05800</td>\n",
       "      <td>1.10500</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.020</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05697</td>\n",
       "      <td>...</td>\n",
       "      <td>19.190</td>\n",
       "      <td>33.88</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.11810</td>\n",
       "      <td>0.15510</td>\n",
       "      <td>0.14590</td>\n",
       "      <td>0.09975</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.08452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15.780</td>\n",
       "      <td>17.89</td>\n",
       "      <td>103.60</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.09710</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.099540</td>\n",
       "      <td>0.066060</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>0.06082</td>\n",
       "      <td>...</td>\n",
       "      <td>20.420</td>\n",
       "      <td>27.28</td>\n",
       "      <td>136.50</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>0.13960</td>\n",
       "      <td>0.56090</td>\n",
       "      <td>0.39650</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.3792</td>\n",
       "      <td>0.10480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19.170</td>\n",
       "      <td>24.80</td>\n",
       "      <td>132.40</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0.24580</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07800</td>\n",
       "      <td>...</td>\n",
       "      <td>20.960</td>\n",
       "      <td>29.94</td>\n",
       "      <td>151.70</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>0.10370</td>\n",
       "      <td>0.39030</td>\n",
       "      <td>0.36390</td>\n",
       "      <td>0.17670</td>\n",
       "      <td>0.3176</td>\n",
       "      <td>0.10230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.850</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.70</td>\n",
       "      <td>782.7</td>\n",
       "      <td>0.08401</td>\n",
       "      <td>0.10020</td>\n",
       "      <td>0.099380</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.05338</td>\n",
       "      <td>...</td>\n",
       "      <td>16.840</td>\n",
       "      <td>27.66</td>\n",
       "      <td>112.00</td>\n",
       "      <td>876.5</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.19240</td>\n",
       "      <td>0.23220</td>\n",
       "      <td>0.11190</td>\n",
       "      <td>0.2809</td>\n",
       "      <td>0.06287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.730</td>\n",
       "      <td>22.61</td>\n",
       "      <td>93.60</td>\n",
       "      <td>578.3</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.22930</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.080250</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.07682</td>\n",
       "      <td>...</td>\n",
       "      <td>15.030</td>\n",
       "      <td>32.01</td>\n",
       "      <td>108.80</td>\n",
       "      <td>697.7</td>\n",
       "      <td>0.16510</td>\n",
       "      <td>0.77250</td>\n",
       "      <td>0.69430</td>\n",
       "      <td>0.22080</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.14310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.540</td>\n",
       "      <td>27.54</td>\n",
       "      <td>96.73</td>\n",
       "      <td>658.8</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.15950</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.073640</td>\n",
       "      <td>0.2303</td>\n",
       "      <td>0.07077</td>\n",
       "      <td>...</td>\n",
       "      <td>17.460</td>\n",
       "      <td>37.13</td>\n",
       "      <td>124.10</td>\n",
       "      <td>943.2</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.65770</td>\n",
       "      <td>0.70260</td>\n",
       "      <td>0.17120</td>\n",
       "      <td>0.4218</td>\n",
       "      <td>0.13410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.680</td>\n",
       "      <td>20.13</td>\n",
       "      <td>94.74</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.09867</td>\n",
       "      <td>0.07200</td>\n",
       "      <td>0.073950</td>\n",
       "      <td>0.052590</td>\n",
       "      <td>0.1586</td>\n",
       "      <td>0.05922</td>\n",
       "      <td>...</td>\n",
       "      <td>19.070</td>\n",
       "      <td>30.88</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.14640</td>\n",
       "      <td>0.18710</td>\n",
       "      <td>0.29140</td>\n",
       "      <td>0.16090</td>\n",
       "      <td>0.3029</td>\n",
       "      <td>0.08216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.130</td>\n",
       "      <td>20.68</td>\n",
       "      <td>108.10</td>\n",
       "      <td>798.8</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.20220</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.07356</td>\n",
       "      <td>...</td>\n",
       "      <td>20.960</td>\n",
       "      <td>31.48</td>\n",
       "      <td>136.80</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>0.17890</td>\n",
       "      <td>0.42330</td>\n",
       "      <td>0.47840</td>\n",
       "      <td>0.20730</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>0.11420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.810</td>\n",
       "      <td>22.15</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.10270</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.094980</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.05395</td>\n",
       "      <td>...</td>\n",
       "      <td>27.320</td>\n",
       "      <td>30.88</td>\n",
       "      <td>186.80</td>\n",
       "      <td>2398.0</td>\n",
       "      <td>0.15120</td>\n",
       "      <td>0.31500</td>\n",
       "      <td>0.53720</td>\n",
       "      <td>0.23880</td>\n",
       "      <td>0.2768</td>\n",
       "      <td>0.07615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.540</td>\n",
       "      <td>14.36</td>\n",
       "      <td>87.46</td>\n",
       "      <td>566.3</td>\n",
       "      <td>0.09779</td>\n",
       "      <td>0.08129</td>\n",
       "      <td>0.066640</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.05766</td>\n",
       "      <td>...</td>\n",
       "      <td>15.110</td>\n",
       "      <td>19.26</td>\n",
       "      <td>99.70</td>\n",
       "      <td>711.2</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.17730</td>\n",
       "      <td>0.23900</td>\n",
       "      <td>0.12880</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.07259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.045680</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.06811</td>\n",
       "      <td>...</td>\n",
       "      <td>14.500</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.18900</td>\n",
       "      <td>0.07283</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.08183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>...</td>\n",
       "      <td>10.230</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.13240</td>\n",
       "      <td>0.11480</td>\n",
       "      <td>0.08867</td>\n",
       "      <td>0.06227</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.07773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.340</td>\n",
       "      <td>14.26</td>\n",
       "      <td>102.50</td>\n",
       "      <td>704.4</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.21350</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>0.097560</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.07032</td>\n",
       "      <td>...</td>\n",
       "      <td>18.070</td>\n",
       "      <td>19.08</td>\n",
       "      <td>125.10</td>\n",
       "      <td>980.9</td>\n",
       "      <td>0.13900</td>\n",
       "      <td>0.59540</td>\n",
       "      <td>0.63050</td>\n",
       "      <td>0.23930</td>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.09946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21.160</td>\n",
       "      <td>23.04</td>\n",
       "      <td>137.20</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>0.09428</td>\n",
       "      <td>0.10220</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.086320</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.05278</td>\n",
       "      <td>...</td>\n",
       "      <td>29.170</td>\n",
       "      <td>35.59</td>\n",
       "      <td>188.00</td>\n",
       "      <td>2615.0</td>\n",
       "      <td>0.14010</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>0.31550</td>\n",
       "      <td>0.20090</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.07526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16.650</td>\n",
       "      <td>21.38</td>\n",
       "      <td>110.00</td>\n",
       "      <td>904.6</td>\n",
       "      <td>0.11210</td>\n",
       "      <td>0.14570</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.06330</td>\n",
       "      <td>...</td>\n",
       "      <td>26.460</td>\n",
       "      <td>31.56</td>\n",
       "      <td>177.00</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>0.18050</td>\n",
       "      <td>0.35780</td>\n",
       "      <td>0.46950</td>\n",
       "      <td>0.20950</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.09564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.140</td>\n",
       "      <td>16.40</td>\n",
       "      <td>116.00</td>\n",
       "      <td>912.7</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.22760</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>0.07413</td>\n",
       "      <td>...</td>\n",
       "      <td>22.250</td>\n",
       "      <td>21.40</td>\n",
       "      <td>152.40</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.15450</td>\n",
       "      <td>0.39490</td>\n",
       "      <td>0.38530</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.10590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.580</td>\n",
       "      <td>21.53</td>\n",
       "      <td>97.41</td>\n",
       "      <td>644.8</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.18680</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.087830</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>0.06924</td>\n",
       "      <td>...</td>\n",
       "      <td>17.620</td>\n",
       "      <td>33.21</td>\n",
       "      <td>122.40</td>\n",
       "      <td>896.9</td>\n",
       "      <td>0.15250</td>\n",
       "      <td>0.66430</td>\n",
       "      <td>0.55390</td>\n",
       "      <td>0.27010</td>\n",
       "      <td>0.4264</td>\n",
       "      <td>0.12750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18.610</td>\n",
       "      <td>20.25</td>\n",
       "      <td>122.10</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>0.09440</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.077310</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.05699</td>\n",
       "      <td>...</td>\n",
       "      <td>21.310</td>\n",
       "      <td>27.26</td>\n",
       "      <td>139.90</td>\n",
       "      <td>1403.0</td>\n",
       "      <td>0.13380</td>\n",
       "      <td>0.21170</td>\n",
       "      <td>0.34460</td>\n",
       "      <td>0.14900</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.07421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15.300</td>\n",
       "      <td>25.27</td>\n",
       "      <td>102.40</td>\n",
       "      <td>732.4</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>0.16970</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.087510</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>...</td>\n",
       "      <td>20.270</td>\n",
       "      <td>36.71</td>\n",
       "      <td>149.30</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>0.16410</td>\n",
       "      <td>0.61100</td>\n",
       "      <td>0.63350</td>\n",
       "      <td>0.20240</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>0.09876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17.570</td>\n",
       "      <td>15.05</td>\n",
       "      <td>115.00</td>\n",
       "      <td>955.1</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.11570</td>\n",
       "      <td>0.098750</td>\n",
       "      <td>0.079530</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.06149</td>\n",
       "      <td>...</td>\n",
       "      <td>20.010</td>\n",
       "      <td>19.52</td>\n",
       "      <td>134.90</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.28120</td>\n",
       "      <td>0.24890</td>\n",
       "      <td>0.14560</td>\n",
       "      <td>0.2756</td>\n",
       "      <td>0.07919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>7.691</td>\n",
       "      <td>25.44</td>\n",
       "      <td>48.34</td>\n",
       "      <td>170.4</td>\n",
       "      <td>0.08668</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.092520</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>0.2037</td>\n",
       "      <td>0.07751</td>\n",
       "      <td>...</td>\n",
       "      <td>8.678</td>\n",
       "      <td>31.89</td>\n",
       "      <td>54.49</td>\n",
       "      <td>223.6</td>\n",
       "      <td>0.15960</td>\n",
       "      <td>0.30640</td>\n",
       "      <td>0.33930</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.10660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>11.540</td>\n",
       "      <td>14.44</td>\n",
       "      <td>74.65</td>\n",
       "      <td>402.9</td>\n",
       "      <td>0.09984</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.067370</td>\n",
       "      <td>0.025940</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.06782</td>\n",
       "      <td>...</td>\n",
       "      <td>12.260</td>\n",
       "      <td>19.68</td>\n",
       "      <td>78.78</td>\n",
       "      <td>457.8</td>\n",
       "      <td>0.13450</td>\n",
       "      <td>0.21180</td>\n",
       "      <td>0.17970</td>\n",
       "      <td>0.06918</td>\n",
       "      <td>0.2329</td>\n",
       "      <td>0.08134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>14.470</td>\n",
       "      <td>24.99</td>\n",
       "      <td>95.81</td>\n",
       "      <td>656.4</td>\n",
       "      <td>0.08837</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>0.06341</td>\n",
       "      <td>...</td>\n",
       "      <td>16.220</td>\n",
       "      <td>31.73</td>\n",
       "      <td>113.50</td>\n",
       "      <td>808.9</td>\n",
       "      <td>0.13400</td>\n",
       "      <td>0.42020</td>\n",
       "      <td>0.40400</td>\n",
       "      <td>0.12050</td>\n",
       "      <td>0.3187</td>\n",
       "      <td>0.10230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>14.740</td>\n",
       "      <td>25.42</td>\n",
       "      <td>94.70</td>\n",
       "      <td>668.6</td>\n",
       "      <td>0.08275</td>\n",
       "      <td>0.07214</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.030270</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.05680</td>\n",
       "      <td>...</td>\n",
       "      <td>16.510</td>\n",
       "      <td>32.29</td>\n",
       "      <td>107.40</td>\n",
       "      <td>826.4</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.13760</td>\n",
       "      <td>0.16110</td>\n",
       "      <td>0.10950</td>\n",
       "      <td>0.2722</td>\n",
       "      <td>0.06956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>13.210</td>\n",
       "      <td>28.06</td>\n",
       "      <td>84.88</td>\n",
       "      <td>538.4</td>\n",
       "      <td>0.08671</td>\n",
       "      <td>0.06877</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.032750</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.05781</td>\n",
       "      <td>...</td>\n",
       "      <td>14.370</td>\n",
       "      <td>37.17</td>\n",
       "      <td>92.48</td>\n",
       "      <td>629.6</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.13810</td>\n",
       "      <td>0.10620</td>\n",
       "      <td>0.07958</td>\n",
       "      <td>0.2473</td>\n",
       "      <td>0.06443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>13.870</td>\n",
       "      <td>20.70</td>\n",
       "      <td>89.77</td>\n",
       "      <td>584.8</td>\n",
       "      <td>0.09578</td>\n",
       "      <td>0.10180</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.023690</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>0.06688</td>\n",
       "      <td>...</td>\n",
       "      <td>15.050</td>\n",
       "      <td>24.75</td>\n",
       "      <td>99.17</td>\n",
       "      <td>688.6</td>\n",
       "      <td>0.12640</td>\n",
       "      <td>0.20370</td>\n",
       "      <td>0.13770</td>\n",
       "      <td>0.06845</td>\n",
       "      <td>0.2249</td>\n",
       "      <td>0.08492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>13.620</td>\n",
       "      <td>23.23</td>\n",
       "      <td>87.19</td>\n",
       "      <td>573.2</td>\n",
       "      <td>0.09246</td>\n",
       "      <td>0.06747</td>\n",
       "      <td>0.029740</td>\n",
       "      <td>0.024430</td>\n",
       "      <td>0.1664</td>\n",
       "      <td>0.05801</td>\n",
       "      <td>...</td>\n",
       "      <td>15.350</td>\n",
       "      <td>29.09</td>\n",
       "      <td>97.58</td>\n",
       "      <td>729.8</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.15170</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>0.07174</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.06953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>10.320</td>\n",
       "      <td>16.35</td>\n",
       "      <td>65.31</td>\n",
       "      <td>324.9</td>\n",
       "      <td>0.09434</td>\n",
       "      <td>0.04994</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.06201</td>\n",
       "      <td>...</td>\n",
       "      <td>11.250</td>\n",
       "      <td>21.77</td>\n",
       "      <td>71.12</td>\n",
       "      <td>384.9</td>\n",
       "      <td>0.12850</td>\n",
       "      <td>0.08842</td>\n",
       "      <td>0.04384</td>\n",
       "      <td>0.02381</td>\n",
       "      <td>0.2681</td>\n",
       "      <td>0.07399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>10.260</td>\n",
       "      <td>16.58</td>\n",
       "      <td>65.85</td>\n",
       "      <td>320.8</td>\n",
       "      <td>0.08877</td>\n",
       "      <td>0.08066</td>\n",
       "      <td>0.043580</td>\n",
       "      <td>0.024380</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.06714</td>\n",
       "      <td>...</td>\n",
       "      <td>10.830</td>\n",
       "      <td>22.04</td>\n",
       "      <td>71.08</td>\n",
       "      <td>357.4</td>\n",
       "      <td>0.14610</td>\n",
       "      <td>0.22460</td>\n",
       "      <td>0.17830</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.2691</td>\n",
       "      <td>0.09479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>9.683</td>\n",
       "      <td>19.34</td>\n",
       "      <td>61.05</td>\n",
       "      <td>285.7</td>\n",
       "      <td>0.08491</td>\n",
       "      <td>0.05030</td>\n",
       "      <td>0.023370</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.06235</td>\n",
       "      <td>...</td>\n",
       "      <td>10.930</td>\n",
       "      <td>25.59</td>\n",
       "      <td>69.10</td>\n",
       "      <td>364.2</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.09546</td>\n",
       "      <td>0.09350</td>\n",
       "      <td>0.03846</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>0.07920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>10.820</td>\n",
       "      <td>24.21</td>\n",
       "      <td>68.89</td>\n",
       "      <td>361.6</td>\n",
       "      <td>0.08192</td>\n",
       "      <td>0.06602</td>\n",
       "      <td>0.015480</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.1976</td>\n",
       "      <td>0.06328</td>\n",
       "      <td>...</td>\n",
       "      <td>13.030</td>\n",
       "      <td>31.45</td>\n",
       "      <td>83.90</td>\n",
       "      <td>505.6</td>\n",
       "      <td>0.12040</td>\n",
       "      <td>0.16330</td>\n",
       "      <td>0.06194</td>\n",
       "      <td>0.03264</td>\n",
       "      <td>0.3059</td>\n",
       "      <td>0.07626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>10.860</td>\n",
       "      <td>21.48</td>\n",
       "      <td>68.51</td>\n",
       "      <td>360.5</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.04227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1661</td>\n",
       "      <td>0.05948</td>\n",
       "      <td>...</td>\n",
       "      <td>11.660</td>\n",
       "      <td>24.77</td>\n",
       "      <td>74.08</td>\n",
       "      <td>412.3</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.07348</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>0.06592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>11.130</td>\n",
       "      <td>22.44</td>\n",
       "      <td>71.49</td>\n",
       "      <td>378.4</td>\n",
       "      <td>0.09566</td>\n",
       "      <td>0.08194</td>\n",
       "      <td>0.048240</td>\n",
       "      <td>0.022570</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.06552</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020</td>\n",
       "      <td>28.26</td>\n",
       "      <td>77.80</td>\n",
       "      <td>436.6</td>\n",
       "      <td>0.10870</td>\n",
       "      <td>0.17820</td>\n",
       "      <td>0.15640</td>\n",
       "      <td>0.06413</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.08032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>12.770</td>\n",
       "      <td>29.43</td>\n",
       "      <td>81.35</td>\n",
       "      <td>507.9</td>\n",
       "      <td>0.08276</td>\n",
       "      <td>0.04234</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.05637</td>\n",
       "      <td>...</td>\n",
       "      <td>13.870</td>\n",
       "      <td>36.00</td>\n",
       "      <td>88.10</td>\n",
       "      <td>594.7</td>\n",
       "      <td>0.12340</td>\n",
       "      <td>0.10640</td>\n",
       "      <td>0.08653</td>\n",
       "      <td>0.06498</td>\n",
       "      <td>0.2407</td>\n",
       "      <td>0.06484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>9.333</td>\n",
       "      <td>21.94</td>\n",
       "      <td>59.01</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.09240</td>\n",
       "      <td>0.05605</td>\n",
       "      <td>0.039960</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>0.1692</td>\n",
       "      <td>0.06576</td>\n",
       "      <td>...</td>\n",
       "      <td>9.845</td>\n",
       "      <td>25.05</td>\n",
       "      <td>62.86</td>\n",
       "      <td>295.8</td>\n",
       "      <td>0.11030</td>\n",
       "      <td>0.08298</td>\n",
       "      <td>0.07993</td>\n",
       "      <td>0.02564</td>\n",
       "      <td>0.2435</td>\n",
       "      <td>0.07393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>12.880</td>\n",
       "      <td>28.92</td>\n",
       "      <td>82.50</td>\n",
       "      <td>514.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.05824</td>\n",
       "      <td>0.061950</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05708</td>\n",
       "      <td>...</td>\n",
       "      <td>13.890</td>\n",
       "      <td>35.74</td>\n",
       "      <td>88.84</td>\n",
       "      <td>595.7</td>\n",
       "      <td>0.12270</td>\n",
       "      <td>0.16200</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.06493</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.07242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>10.290</td>\n",
       "      <td>27.61</td>\n",
       "      <td>65.67</td>\n",
       "      <td>321.4</td>\n",
       "      <td>0.09030</td>\n",
       "      <td>0.07658</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.06127</td>\n",
       "      <td>...</td>\n",
       "      <td>10.840</td>\n",
       "      <td>34.91</td>\n",
       "      <td>69.57</td>\n",
       "      <td>357.6</td>\n",
       "      <td>0.13840</td>\n",
       "      <td>0.17100</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.09127</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.08283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>10.160</td>\n",
       "      <td>19.59</td>\n",
       "      <td>64.73</td>\n",
       "      <td>311.7</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.07504</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.06331</td>\n",
       "      <td>...</td>\n",
       "      <td>10.650</td>\n",
       "      <td>22.88</td>\n",
       "      <td>67.88</td>\n",
       "      <td>347.3</td>\n",
       "      <td>0.12650</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>0.01005</td>\n",
       "      <td>0.02232</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.06742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>9.423</td>\n",
       "      <td>27.88</td>\n",
       "      <td>59.26</td>\n",
       "      <td>271.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.04971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.06059</td>\n",
       "      <td>...</td>\n",
       "      <td>10.490</td>\n",
       "      <td>34.24</td>\n",
       "      <td>66.50</td>\n",
       "      <td>330.6</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.07158</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>0.06969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>14.590</td>\n",
       "      <td>22.68</td>\n",
       "      <td>96.39</td>\n",
       "      <td>657.1</td>\n",
       "      <td>0.08473</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.06147</td>\n",
       "      <td>...</td>\n",
       "      <td>15.480</td>\n",
       "      <td>27.27</td>\n",
       "      <td>105.90</td>\n",
       "      <td>733.5</td>\n",
       "      <td>0.10260</td>\n",
       "      <td>0.31710</td>\n",
       "      <td>0.36620</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.08004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>11.510</td>\n",
       "      <td>23.93</td>\n",
       "      <td>74.52</td>\n",
       "      <td>403.5</td>\n",
       "      <td>0.09261</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.06570</td>\n",
       "      <td>...</td>\n",
       "      <td>12.480</td>\n",
       "      <td>37.16</td>\n",
       "      <td>82.28</td>\n",
       "      <td>474.2</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.09653</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>0.08732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>14.050</td>\n",
       "      <td>27.15</td>\n",
       "      <td>91.38</td>\n",
       "      <td>600.4</td>\n",
       "      <td>0.09929</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>0.06171</td>\n",
       "      <td>...</td>\n",
       "      <td>15.300</td>\n",
       "      <td>33.17</td>\n",
       "      <td>100.20</td>\n",
       "      <td>706.7</td>\n",
       "      <td>0.12410</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.13260</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.08321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>11.200</td>\n",
       "      <td>29.37</td>\n",
       "      <td>70.67</td>\n",
       "      <td>386.0</td>\n",
       "      <td>0.07449</td>\n",
       "      <td>0.03558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.05502</td>\n",
       "      <td>...</td>\n",
       "      <td>11.920</td>\n",
       "      <td>38.30</td>\n",
       "      <td>75.19</td>\n",
       "      <td>439.6</td>\n",
       "      <td>0.09267</td>\n",
       "      <td>0.05494</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>15.220</td>\n",
       "      <td>30.62</td>\n",
       "      <td>103.40</td>\n",
       "      <td>716.9</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.094290</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.07152</td>\n",
       "      <td>...</td>\n",
       "      <td>17.520</td>\n",
       "      <td>42.79</td>\n",
       "      <td>128.70</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0.14170</td>\n",
       "      <td>0.79170</td>\n",
       "      <td>1.17000</td>\n",
       "      <td>0.23560</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>0.14090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>20.920</td>\n",
       "      <td>25.09</td>\n",
       "      <td>143.00</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.22360</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.2149</td>\n",
       "      <td>0.06879</td>\n",
       "      <td>...</td>\n",
       "      <td>24.290</td>\n",
       "      <td>29.41</td>\n",
       "      <td>179.10</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.14070</td>\n",
       "      <td>0.41860</td>\n",
       "      <td>0.65990</td>\n",
       "      <td>0.25420</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>0.09873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.560</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.41070</td>\n",
       "      <td>0.22160</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.130</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.097910</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.32150</td>\n",
       "      <td>0.16280</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.600</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.092510</td>\n",
       "      <td>0.053020</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.34030</td>\n",
       "      <td>0.14180</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.600</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.93870</td>\n",
       "      <td>0.26500</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.760</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1       2       3        4        5         6         7   \\\n",
       "0    17.990  10.38  122.80  1001.0  0.11840  0.27760  0.300100  0.147100   \n",
       "1    20.570  17.77  132.90  1326.0  0.08474  0.07864  0.086900  0.070170   \n",
       "2    19.690  21.25  130.00  1203.0  0.10960  0.15990  0.197400  0.127900   \n",
       "3    11.420  20.38   77.58   386.1  0.14250  0.28390  0.241400  0.105200   \n",
       "4    20.290  14.34  135.10  1297.0  0.10030  0.13280  0.198000  0.104300   \n",
       "5    12.450  15.70   82.57   477.1  0.12780  0.17000  0.157800  0.080890   \n",
       "6    18.250  19.98  119.60  1040.0  0.09463  0.10900  0.112700  0.074000   \n",
       "7    13.710  20.83   90.20   577.9  0.11890  0.16450  0.093660  0.059850   \n",
       "8    13.000  21.82   87.50   519.8  0.12730  0.19320  0.185900  0.093530   \n",
       "9    12.460  24.04   83.97   475.9  0.11860  0.23960  0.227300  0.085430   \n",
       "10   16.020  23.24  102.70   797.8  0.08206  0.06669  0.032990  0.033230   \n",
       "11   15.780  17.89  103.60   781.0  0.09710  0.12920  0.099540  0.066060   \n",
       "12   19.170  24.80  132.40  1123.0  0.09740  0.24580  0.206500  0.111800   \n",
       "13   15.850  23.95  103.70   782.7  0.08401  0.10020  0.099380  0.053640   \n",
       "14   13.730  22.61   93.60   578.3  0.11310  0.22930  0.212800  0.080250   \n",
       "15   14.540  27.54   96.73   658.8  0.11390  0.15950  0.163900  0.073640   \n",
       "16   14.680  20.13   94.74   684.5  0.09867  0.07200  0.073950  0.052590   \n",
       "17   16.130  20.68  108.10   798.8  0.11700  0.20220  0.172200  0.102800   \n",
       "18   19.810  22.15  130.00  1260.0  0.09831  0.10270  0.147900  0.094980   \n",
       "19   13.540  14.36   87.46   566.3  0.09779  0.08129  0.066640  0.047810   \n",
       "20   13.080  15.71   85.63   520.0  0.10750  0.12700  0.045680  0.031100   \n",
       "21    9.504  12.44   60.34   273.9  0.10240  0.06492  0.029560  0.020760   \n",
       "22   15.340  14.26  102.50   704.4  0.10730  0.21350  0.207700  0.097560   \n",
       "23   21.160  23.04  137.20  1404.0  0.09428  0.10220  0.109700  0.086320   \n",
       "24   16.650  21.38  110.00   904.6  0.11210  0.14570  0.152500  0.091700   \n",
       "25   17.140  16.40  116.00   912.7  0.11860  0.22760  0.222900  0.140100   \n",
       "26   14.580  21.53   97.41   644.8  0.10540  0.18680  0.142500  0.087830   \n",
       "27   18.610  20.25  122.10  1094.0  0.09440  0.10660  0.149000  0.077310   \n",
       "28   15.300  25.27  102.40   732.4  0.10820  0.16970  0.168300  0.087510   \n",
       "29   17.570  15.05  115.00   955.1  0.09847  0.11570  0.098750  0.079530   \n",
       "..      ...    ...     ...     ...      ...      ...       ...       ...   \n",
       "539   7.691  25.44   48.34   170.4  0.08668  0.11990  0.092520  0.013640   \n",
       "540  11.540  14.44   74.65   402.9  0.09984  0.11200  0.067370  0.025940   \n",
       "541  14.470  24.99   95.81   656.4  0.08837  0.12300  0.100900  0.038900   \n",
       "542  14.740  25.42   94.70   668.6  0.08275  0.07214  0.041050  0.030270   \n",
       "543  13.210  28.06   84.88   538.4  0.08671  0.06877  0.029870  0.032750   \n",
       "544  13.870  20.70   89.77   584.8  0.09578  0.10180  0.036880  0.023690   \n",
       "545  13.620  23.23   87.19   573.2  0.09246  0.06747  0.029740  0.024430   \n",
       "546  10.320  16.35   65.31   324.9  0.09434  0.04994  0.010120  0.005495   \n",
       "547  10.260  16.58   65.85   320.8  0.08877  0.08066  0.043580  0.024380   \n",
       "548   9.683  19.34   61.05   285.7  0.08491  0.05030  0.023370  0.009615   \n",
       "549  10.820  24.21   68.89   361.6  0.08192  0.06602  0.015480  0.008160   \n",
       "550  10.860  21.48   68.51   360.5  0.07431  0.04227  0.000000  0.000000   \n",
       "551  11.130  22.44   71.49   378.4  0.09566  0.08194  0.048240  0.022570   \n",
       "552  12.770  29.43   81.35   507.9  0.08276  0.04234  0.019970  0.014990   \n",
       "553   9.333  21.94   59.01   264.0  0.09240  0.05605  0.039960  0.012820   \n",
       "554  12.880  28.92   82.50   514.3  0.08123  0.05824  0.061950  0.023430   \n",
       "555  10.290  27.61   65.67   321.4  0.09030  0.07658  0.059990  0.027380   \n",
       "556  10.160  19.59   64.73   311.7  0.10030  0.07504  0.005025  0.011160   \n",
       "557   9.423  27.88   59.26   271.3  0.08123  0.04971  0.000000  0.000000   \n",
       "558  14.590  22.68   96.39   657.1  0.08473  0.13300  0.102900  0.037360   \n",
       "559  11.510  23.93   74.52   403.5  0.09261  0.10210  0.111200  0.041050   \n",
       "560  14.050  27.15   91.38   600.4  0.09929  0.11260  0.044620  0.043040   \n",
       "561  11.200  29.37   70.67   386.0  0.07449  0.03558  0.000000  0.000000   \n",
       "562  15.220  30.62  103.40   716.9  0.10480  0.20870  0.255000  0.094290   \n",
       "563  20.920  25.09  143.00  1347.0  0.10990  0.22360  0.317400  0.147400   \n",
       "564  21.560  22.39  142.00  1479.0  0.11100  0.11590  0.243900  0.138900   \n",
       "565  20.130  28.25  131.20  1261.0  0.09780  0.10340  0.144000  0.097910   \n",
       "566  16.600  28.08  108.30   858.1  0.08455  0.10230  0.092510  0.053020   \n",
       "567  20.600  29.33  140.10  1265.0  0.11780  0.27700  0.351400  0.152000   \n",
       "568   7.760  24.54   47.92   181.0  0.05263  0.04362  0.000000  0.000000   \n",
       "\n",
       "         8        9    ...         20     21      22      23       24  \\\n",
       "0    0.2419  0.07871   ...     25.380  17.33  184.60  2019.0  0.16220   \n",
       "1    0.1812  0.05667   ...     24.990  23.41  158.80  1956.0  0.12380   \n",
       "2    0.2069  0.05999   ...     23.570  25.53  152.50  1709.0  0.14440   \n",
       "3    0.2597  0.09744   ...     14.910  26.50   98.87   567.7  0.20980   \n",
       "4    0.1809  0.05883   ...     22.540  16.67  152.20  1575.0  0.13740   \n",
       "5    0.2087  0.07613   ...     15.470  23.75  103.40   741.6  0.17910   \n",
       "6    0.1794  0.05742   ...     22.880  27.66  153.20  1606.0  0.14420   \n",
       "7    0.2196  0.07451   ...     17.060  28.14  110.60   897.0  0.16540   \n",
       "8    0.2350  0.07389   ...     15.490  30.73  106.20   739.3  0.17030   \n",
       "9    0.2030  0.08243   ...     15.090  40.68   97.65   711.4  0.18530   \n",
       "10   0.1528  0.05697   ...     19.190  33.88  123.80  1150.0  0.11810   \n",
       "11   0.1842  0.06082   ...     20.420  27.28  136.50  1299.0  0.13960   \n",
       "12   0.2397  0.07800   ...     20.960  29.94  151.70  1332.0  0.10370   \n",
       "13   0.1847  0.05338   ...     16.840  27.66  112.00   876.5  0.11310   \n",
       "14   0.2069  0.07682   ...     15.030  32.01  108.80   697.7  0.16510   \n",
       "15   0.2303  0.07077   ...     17.460  37.13  124.10   943.2  0.16780   \n",
       "16   0.1586  0.05922   ...     19.070  30.88  123.40  1138.0  0.14640   \n",
       "17   0.2164  0.07356   ...     20.960  31.48  136.80  1315.0  0.17890   \n",
       "18   0.1582  0.05395   ...     27.320  30.88  186.80  2398.0  0.15120   \n",
       "19   0.1885  0.05766   ...     15.110  19.26   99.70   711.2  0.14400   \n",
       "20   0.1967  0.06811   ...     14.500  20.49   96.09   630.5  0.13120   \n",
       "21   0.1815  0.06905   ...     10.230  15.66   65.13   314.9  0.13240   \n",
       "22   0.2521  0.07032   ...     18.070  19.08  125.10   980.9  0.13900   \n",
       "23   0.1769  0.05278   ...     29.170  35.59  188.00  2615.0  0.14010   \n",
       "24   0.1995  0.06330   ...     26.460  31.56  177.00  2215.0  0.18050   \n",
       "25   0.3040  0.07413   ...     22.250  21.40  152.40  1461.0  0.15450   \n",
       "26   0.2252  0.06924   ...     17.620  33.21  122.40   896.9  0.15250   \n",
       "27   0.1697  0.05699   ...     21.310  27.26  139.90  1403.0  0.13380   \n",
       "28   0.1926  0.06540   ...     20.270  36.71  149.30  1269.0  0.16410   \n",
       "29   0.1739  0.06149   ...     20.010  19.52  134.90  1227.0  0.12550   \n",
       "..      ...      ...   ...        ...    ...     ...     ...      ...   \n",
       "539  0.2037  0.07751   ...      8.678  31.89   54.49   223.6  0.15960   \n",
       "540  0.1818  0.06782   ...     12.260  19.68   78.78   457.8  0.13450   \n",
       "541  0.1872  0.06341   ...     16.220  31.73  113.50   808.9  0.13400   \n",
       "542  0.1840  0.05680   ...     16.510  32.29  107.40   826.4  0.10600   \n",
       "543  0.1628  0.05781   ...     14.370  37.17   92.48   629.6  0.10720   \n",
       "544  0.1620  0.06688   ...     15.050  24.75   99.17   688.6  0.12640   \n",
       "545  0.1664  0.05801   ...     15.350  29.09   97.58   729.8  0.12160   \n",
       "546  0.1885  0.06201   ...     11.250  21.77   71.12   384.9  0.12850   \n",
       "547  0.1669  0.06714   ...     10.830  22.04   71.08   357.4  0.14610   \n",
       "548  0.1580  0.06235   ...     10.930  25.59   69.10   364.2  0.11990   \n",
       "549  0.1976  0.06328   ...     13.030  31.45   83.90   505.6  0.12040   \n",
       "550  0.1661  0.05948   ...     11.660  24.77   74.08   412.3  0.10010   \n",
       "551  0.2030  0.06552   ...     12.020  28.26   77.80   436.6  0.10870   \n",
       "552  0.1539  0.05637   ...     13.870  36.00   88.10   594.7  0.12340   \n",
       "553  0.1692  0.06576   ...      9.845  25.05   62.86   295.8  0.11030   \n",
       "554  0.1566  0.05708   ...     13.890  35.74   88.84   595.7  0.12270   \n",
       "555  0.1593  0.06127   ...     10.840  34.91   69.57   357.6  0.13840   \n",
       "556  0.1791  0.06331   ...     10.650  22.88   67.88   347.3  0.12650   \n",
       "557  0.1742  0.06059   ...     10.490  34.24   66.50   330.6  0.10730   \n",
       "558  0.1454  0.06147   ...     15.480  27.27  105.90   733.5  0.10260   \n",
       "559  0.1388  0.06570   ...     12.480  37.16   82.28   474.2  0.12980   \n",
       "560  0.1537  0.06171   ...     15.300  33.17  100.20   706.7  0.12410   \n",
       "561  0.1060  0.05502   ...     11.920  38.30   75.19   439.6  0.09267   \n",
       "562  0.2128  0.07152   ...     17.520  42.79  128.70   915.0  0.14170   \n",
       "563  0.2149  0.06879   ...     24.290  29.41  179.10  1819.0  0.14070   \n",
       "564  0.1726  0.05623   ...     25.450  26.40  166.10  2027.0  0.14100   \n",
       "565  0.1752  0.05533   ...     23.690  38.25  155.00  1731.0  0.11660   \n",
       "566  0.1590  0.05648   ...     18.980  34.12  126.70  1124.0  0.11390   \n",
       "567  0.2397  0.07016   ...     25.740  39.42  184.60  1821.0  0.16500   \n",
       "568  0.1587  0.05884   ...      9.456  30.37   59.16   268.6  0.08996   \n",
       "\n",
       "          25       26       27      28       29  \n",
       "0    0.66560  0.71190  0.26540  0.4601  0.11890  \n",
       "1    0.18660  0.24160  0.18600  0.2750  0.08902  \n",
       "2    0.42450  0.45040  0.24300  0.3613  0.08758  \n",
       "3    0.86630  0.68690  0.25750  0.6638  0.17300  \n",
       "4    0.20500  0.40000  0.16250  0.2364  0.07678  \n",
       "5    0.52490  0.53550  0.17410  0.3985  0.12440  \n",
       "6    0.25760  0.37840  0.19320  0.3063  0.08368  \n",
       "7    0.36820  0.26780  0.15560  0.3196  0.11510  \n",
       "8    0.54010  0.53900  0.20600  0.4378  0.10720  \n",
       "9    1.05800  1.10500  0.22100  0.4366  0.20750  \n",
       "10   0.15510  0.14590  0.09975  0.2948  0.08452  \n",
       "11   0.56090  0.39650  0.18100  0.3792  0.10480  \n",
       "12   0.39030  0.36390  0.17670  0.3176  0.10230  \n",
       "13   0.19240  0.23220  0.11190  0.2809  0.06287  \n",
       "14   0.77250  0.69430  0.22080  0.3596  0.14310  \n",
       "15   0.65770  0.70260  0.17120  0.4218  0.13410  \n",
       "16   0.18710  0.29140  0.16090  0.3029  0.08216  \n",
       "17   0.42330  0.47840  0.20730  0.3706  0.11420  \n",
       "18   0.31500  0.53720  0.23880  0.2768  0.07615  \n",
       "19   0.17730  0.23900  0.12880  0.2977  0.07259  \n",
       "20   0.27760  0.18900  0.07283  0.3184  0.08183  \n",
       "21   0.11480  0.08867  0.06227  0.2450  0.07773  \n",
       "22   0.59540  0.63050  0.23930  0.4667  0.09946  \n",
       "23   0.26000  0.31550  0.20090  0.2822  0.07526  \n",
       "24   0.35780  0.46950  0.20950  0.3613  0.09564  \n",
       "25   0.39490  0.38530  0.25500  0.4066  0.10590  \n",
       "26   0.66430  0.55390  0.27010  0.4264  0.12750  \n",
       "27   0.21170  0.34460  0.14900  0.2341  0.07421  \n",
       "28   0.61100  0.63350  0.20240  0.4027  0.09876  \n",
       "29   0.28120  0.24890  0.14560  0.2756  0.07919  \n",
       "..       ...      ...      ...     ...      ...  \n",
       "539  0.30640  0.33930  0.05000  0.2790  0.10660  \n",
       "540  0.21180  0.17970  0.06918  0.2329  0.08134  \n",
       "541  0.42020  0.40400  0.12050  0.3187  0.10230  \n",
       "542  0.13760  0.16110  0.10950  0.2722  0.06956  \n",
       "543  0.13810  0.10620  0.07958  0.2473  0.06443  \n",
       "544  0.20370  0.13770  0.06845  0.2249  0.08492  \n",
       "545  0.15170  0.10490  0.07174  0.2642  0.06953  \n",
       "546  0.08842  0.04384  0.02381  0.2681  0.07399  \n",
       "547  0.22460  0.17830  0.08333  0.2691  0.09479  \n",
       "548  0.09546  0.09350  0.03846  0.2552  0.07920  \n",
       "549  0.16330  0.06194  0.03264  0.3059  0.07626  \n",
       "550  0.07348  0.00000  0.00000  0.2458  0.06592  \n",
       "551  0.17820  0.15640  0.06413  0.3169  0.08032  \n",
       "552  0.10640  0.08653  0.06498  0.2407  0.06484  \n",
       "553  0.08298  0.07993  0.02564  0.2435  0.07393  \n",
       "554  0.16200  0.24390  0.06493  0.2372  0.07242  \n",
       "555  0.17100  0.20000  0.09127  0.2226  0.08283  \n",
       "556  0.12000  0.01005  0.02232  0.2262  0.06742  \n",
       "557  0.07158  0.00000  0.00000  0.2475  0.06969  \n",
       "558  0.31710  0.36620  0.11050  0.2258  0.08004  \n",
       "559  0.25170  0.36300  0.09653  0.2112  0.08732  \n",
       "560  0.22640  0.13260  0.10480  0.2250  0.08321  \n",
       "561  0.05494  0.00000  0.00000  0.1566  0.05905  \n",
       "562  0.79170  1.17000  0.23560  0.4089  0.14090  \n",
       "563  0.41860  0.65990  0.25420  0.2929  0.09873  \n",
       "564  0.21130  0.41070  0.22160  0.2060  0.07115  \n",
       "565  0.19220  0.32150  0.16280  0.2572  0.06637  \n",
       "566  0.30940  0.34030  0.14180  0.2218  0.07820  \n",
       "567  0.86810  0.93870  0.26500  0.4087  0.12400  \n",
       "568  0.06444  0.00000  0.00000  0.2871  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X  = load_breast_cancer().data\n",
    "Y  =  load_breast_cancer().target\n",
    "\n",
    "pd.DataFrame(X) # 看一下数据集 量纲差异大 考虑做标准化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LogisticRegression in module sklearn.linear_model._logistic:\n",
      "\n",
      "class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      " |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      " |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      " |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      " |  'sag', 'saga' and 'newton-cg' solvers.)\n",
      " |  \n",
      " |  This class implements regularized logistic regression using the\n",
      " |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      " |  that regularization is applied by default**. It can handle both dense\n",
      " |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      " |  floats for optimal performance; any other input format will be converted\n",
      " |  (and copied).\n",
      " |  \n",
      " |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      " |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      " |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      " |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      " |  'saga' solver.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
      " |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      " |      'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
      " |      only supported by the 'saga' solver. If 'none' (not supported by the\n",
      " |      liblinear solver), no regularization is applied.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      " |  \n",
      " |  dual : bool, default=False\n",
      " |      Dual or primal formulation. Dual formulation is only implemented for\n",
      " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, default=1.0\n",
      " |      Inverse of regularization strength; must be a positive float.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  intercept_scaling : float, default=1\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *class_weight='balanced'*\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      " |      data. See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem.\n",
      " |  \n",
      " |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      " |        'saga' are faster for large ones.\n",
      " |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      " |        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      " |        schemes.\n",
      " |      - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
      " |      - 'liblinear' and 'saga' also handle L1 penalty\n",
      " |      - 'saga' also supports 'elasticnet' penalty\n",
      " |      - 'liblinear' does not support setting ``penalty='none'``\n",
      " |  \n",
      " |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      " |      features with approximately the same scale. You can\n",
      " |      preprocess the data with a scaler from sklearn.preprocessing.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      " |  \n",
      " |  max_iter : int, default=100\n",
      " |      Maximum number of iterations taken for the solvers to converge.\n",
      " |  \n",
      " |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      " |      number for verbosity.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPU cores used when parallelizing over classes if\n",
      " |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      " |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      " |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors.\n",
      " |      See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  l1_ratio : float, default=None\n",
      " |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      " |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      " |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      " |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      " |      combination of L1 and L2.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      " |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      " |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      " |      outcome 0 (False).\n",
      " |  \n",
      " |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      " |      Actual number of iterations for all classes. If binary or multinomial,\n",
      " |      it returns only 1 element. For liblinear solver, only the maximum\n",
      " |      number of iteration across all classes is given.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |  \n",
      " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      " |      the parameter ``loss=\"log\"``).\n",
      " |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon,\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller tol parameter.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      " |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      " |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      " |  \n",
      " |  LIBLINEAR -- A Library for Large Linear Classification\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      " |  \n",
      " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      " |      https://hal.inria.fr/hal-00860051/document\n",
      " |  \n",
      " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      " |      SAGA: A Fast Incremental Gradient Method With Support\n",
      " |      for Non-Strongly Convex Composite Objectives\n",
      " |      https://arxiv.org/abs/1407.0202\n",
      " |  \n",
      " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      " |      methods for logistic regression and maximum entropy models.\n",
      " |      Machine Learning 85(1-2):41-75.\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :])\n",
      " |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      " |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.97...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegression\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,) default=None\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             *sample_weight* support to LogisticRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict logarithm of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is proportional to the signed\n",
      " |      distance of that sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 建模\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as LR \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 怎么看方法的参数 ？  ？ ？ \n",
    "\n",
    "help(LR)\n",
    "\n",
    "# 岭回归和 LASSO 在线性回归里 解决共线性的问题 \n",
    "# 在这里解决的是过拟合问题  \n",
    "\n",
    "# L2 正则化 对应岭回归\n",
    "# L1 正则化 对应LASSO回归  默认是L2 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9560632688927944"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2 = LR (penalty = 'l2',solver = 'liblinear',\n",
    "   C = 0.5 ,\n",
    "   max_iter = 1000).fit(X,Y)\n",
    "\n",
    "lr2.score(X,Y) # 用l1 和 l2 差不多 l1稍微高一点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.61331113e+00,  1.00124606e-01,  4.60084835e-02,\n",
       "        -4.19839426e-03, -9.26228937e-02, -3.00484301e-01,\n",
       "        -4.53250190e-01, -2.19778015e-01, -1.33074668e-01,\n",
       "        -1.92576286e-02,  1.89635811e-02,  8.74998561e-01,\n",
       "         1.32421950e-01, -9.53784315e-02, -9.62972408e-03,\n",
       "        -2.53596204e-02, -5.83890299e-02, -2.67755115e-02,\n",
       "        -2.73846616e-02, -8.05302922e-05,  1.28529688e+00,\n",
       "        -3.00088054e-01, -1.74310770e-01, -2.23545072e-02,\n",
       "        -1.70267493e-01, -8.77272211e-01, -1.15830085e+00,\n",
       "        -4.22526360e-01, -4.12406225e-01, -8.66393364e-02]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 和 l1 的区别的是 影响很小的量也会被保留下来\n",
    "lr2.coef_\n",
    "\n",
    "# 用这么多特征来训练模型肯定复杂  特征太多会过拟合  特征越少泛化能力越强  但也不能太少 那样会欠拟合 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制学习曲线 去看 l1 好 还是l2 好 \n",
    "\n",
    "Xtrain,Xtest,Ytrain,Ytest  = train_test_split(X,Y,test_size = 0.3 , random_state = 420 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看C 在L1 L2 下 训练集和测试集的表现 \n",
    "\n",
    "l1 = []\n",
    "l2 = [] \n",
    "\n",
    "l1test = [] \n",
    "l2test = []\n",
    "\n",
    "\n",
    "for i in np.linspace(0.05,1,20):\n",
    "    # 实例化我的模型 并且训练 \n",
    "    lrl1 = LR(penalty = 'l1',\n",
    "      solver = 'liblinear',\n",
    "      C = i,max_iter = 1000).fit(Xtrain,Ytrain)\n",
    "    \n",
    "    lrl2 = LR(penalty = 'l2',\n",
    "      solver = 'liblinear',\n",
    "      C = i,max_iter = 1000).fit(Xtrain,Ytrain)\n",
    "    \n",
    "    l1.append(lrl1.score(Xtrain,Ytrain))\n",
    "    l2.append(lrl2.score(Xtrain,Ytrain))\n",
    "    \n",
    "    l1test.append(lrl1.score(Xtest,Ytest))\n",
    "    l2test.append(lrl2.score(Xtest,Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = [l1,l2,l1test,l2test]\n",
    "color = ['green','black','lightgreen','gray']\n",
    "label = ['L1','L2','L1test','L2test']\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "for i in range(len(graph)):\n",
    "    plt.plot(np.linspace(0.05,1,20),graph[i],color[i],label = label[i])\n",
    "    \n",
    "plt.legend(loc = 4)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 学习曲线正常是下滑的 \n",
    "\n",
    "# 现在的图上升  因为C是乘法的逆  C越大的时候惩罚力度越小\n",
    "\n",
    "\n",
    "# 上边是训练集 下边是测试集\n",
    "\n",
    "# 惩罚力度大是啥意思 ： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiajia/anaconda3/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jiajia/anaconda3/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/jiajia/anaconda3/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1304e3518>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L2 = [] \n",
    "L2test = []\n",
    "\n",
    "for i in range(1,201,10):\n",
    "    lrl2 = LR(penalty = 'l2' , solver = 'liblinear',C = 0.9 ,max_iter = i ).fit(Xtrain,Ytrain)\n",
    "    \n",
    "    L2.append(lrl2.score(Xtrain,Ytrain))\n",
    "    L2test.append(lrl2.score(Xtest,Ytest))\n",
    "    \n",
    "plt.plot(range(1,201,10),L2,label = 'l2')\n",
    "plt.plot(range(1,201,10),L2test,label = 'l2test')\n",
    "\n",
    "plt.legend(loc = 4 )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络搜索确定最优参数\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.preprocessing import StandardScaler \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(X,columns = load_breast_cancer().feature_names)\n",
    "data['label'] = Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...    worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...            17.33           184.60      2019.0   \n",
       "1                 0.05667  ...            23.41           158.80      1956.0   \n",
       "2                 0.05999  ...            25.53           152.50      1709.0   \n",
       "3                 0.09744  ...            26.50            98.87       567.7   \n",
       "4                 0.05883  ...            16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  label  \n",
       "0          0.4601                  0.11890      0  \n",
       "1          0.2750                  0.08902      0  \n",
       "2          0.3613                  0.08758      0  \n",
       "3          0.6638                  0.17300      0  \n",
       "4          0.2364                  0.07678      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分数据集  \n",
    "\n",
    "Xtrain,Xtest,Ytrain,Ytest  = train_test_split(X,Y,test_size = 0.3 , random_state = 420 )\n",
    "\n",
    "\n",
    "# 对训练集和测试集做标准化  去量纲\n",
    "\n",
    "std = StandardScaler().fit(Xtrain)\n",
    "\n",
    "Xtrain_std = std.transform(Xtrain)\n",
    "Xtest_std = std.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.75121417,  0.5542968 ,  1.7312936 , ...,  1.70807299,\n",
       "         1.80619472, -0.29048034],\n",
       "       [-0.73305283, -0.86310636, -0.74881989, ..., -0.43131391,\n",
       "        -0.35125435, -0.89069414],\n",
       "       [ 0.31184493,  0.74583777,  0.51828946, ...,  2.0424947 ,\n",
       "        -0.05182753,  1.60021918],\n",
       "       ...,\n",
       "       [-1.13956925, -0.92620221, -1.15460904, ..., -1.56135528,\n",
       "         0.05375983, -0.40552565],\n",
       "       [-1.08536706, -1.5774415 , -1.06777889, ..., -0.70189148,\n",
       "         0.06006355, -0.39251148],\n",
       "       [ 1.00442846,  1.19426898,  1.0462517 , ...,  1.16691785,\n",
       "         0.65261348,  2.55806255]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=10000),\n",
       "             param_grid={'C': [0.05, 0.10277777777777777, 0.15555555555555556,\n",
       "                               0.20833333333333331, 0.2611111111111111,\n",
       "                               0.3138888888888889, 0.36666666666666664,\n",
       "                               0.41944444444444445, 0.4722222222222222, 0.525,\n",
       "                               0.5777777777777778, 0.6305555555555556,\n",
       "                               0.6833333333333333, 0.7361111111111112,\n",
       "                               0.788888888888889, 0.8416666666666667,\n",
       "                               0.8944444444444445, 0.9472222222222223, 1.0],\n",
       "                         'solver': ['liblinear', 'sag', 'newton-cg', 'lbfgs']})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 网格搜索\n",
    "\n",
    "# 在L2范式下判断 C 和solver最优值\n",
    "p = {\n",
    "    \n",
    "    'C': list(np.linspace(0.05,1,19)),\n",
    "    'solver' : ['liblinear','sag','newton-cg','lbfgs']\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "model  = LR(penalty = 'l2',\n",
    "           max_iter = 10000)\n",
    "\n",
    "GS = GridSearchCV(model,p,cv =5 )\n",
    "GS.fit(Xtrain_std,Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9874683544303797, {'C': 0.3138888888888889, 'solver': 'sag'})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GS.best_score_ , GS.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.3138888888888889, max_iter=10000, solver='sag')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LR(penalty = 'l2',\n",
    "  max_iter = 10000,\n",
    "  C = GS.best_params_['C'],\n",
    "  solver = GS.best_params_['solver'])\n",
    "# 因为 有三个solver 要求损失函数可导 所以 不能用L1范式\n",
    "\n",
    "model.fit(Xtrain_std,Ytrain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9874371859296482, 0.9649122807017544)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(Xtrain_std,Ytrain),model.score(Xtest_std,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x, y = [], []\n",
    "x_test1, x_test2, x_test3 = [], [], []\n",
    "\n",
    "# 随机生成3种不同分类的点，分别打上标签存在y中\n",
    "for i in range(0, 20):\n",
    "    x1 = random.random()\n",
    "    x2 = random.random()\n",
    "    if x1 + x2 < 1:\n",
    "        x.append([x1, x2, 1])\n",
    "        x_test1.append([x1, x2])\n",
    "        y.append([1, 0, 0])\n",
    "\n",
    "    x.append([x1 * 2, x2 + 1, 1])\n",
    "    x_test2.append([x1 * 2, x2 + 1])\n",
    "    y.append([0, 1, 0])\n",
    "\n",
    "    if x1 > x2:\n",
    "        x.append([x1 + 1, x2, 1])\n",
    "        x_test3.append([x1 + 1, x2])\n",
    "        y.append([0, 0, 1])\n",
    "\n",
    "# 将list转换为numpy array\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "# 学习率\n",
    "lr = 0.5\n",
    "\n",
    "w = np.ones((3, 3))\n",
    "m = len(x)\n",
    "for i in range(0, 1000):\n",
    "    w_gard = 0\n",
    "    for j in range(0, m):\n",
    "        # sigmoid函数分类，这里用到了矩阵乘法\n",
    "        w_gard += -x[j] * np.atleast_2d(1 / (1 + np.exp(-np.matmul(w, x[j]))) - y[j]).T\n",
    "    w += lr * w_gard / m\n",
    "\n",
    "# 画点\n",
    "plt.plot(np.array(x_test1)[:, 0], np.array(x_test1)[:, 1], 'ro')\n",
    "plt.plot(np.array(x_test2)[:, 0], np.array(x_test2)[:, 1], 'gx')\n",
    "plt.plot(np.array(x_test3)[:, 0], np.array(x_test3)[:, 1], 'b.')\n",
    "# 画分类直线\n",
    "plt.plot([0, 2], [-w[0][2] / w[0][1], -(w[0][2] + 2 * w[0][0]) / w[0][1]], 'r')\n",
    "plt.plot([0, 2], [-w[1][2] / w[1][1], -(w[1][2] + 2 * w[1][0]) / w[1][1]], 'g')\n",
    "plt.plot([0, 2], [-w[2][2] / w[2][1], -(w[2][2] + 2 * w[2][0]) / w[2][1]], 'b')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
